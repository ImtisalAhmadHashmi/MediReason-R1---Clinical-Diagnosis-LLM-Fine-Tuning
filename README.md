# MediReason-R1---Clinical-Diagnosis-LLM-Fine-Tuning
üîπ Domain Adaptation & Model Architecture\
‚Ä¢ Fine-tuned DeepSeek-R1 (8B) using 4-bit quantization & LoRA (r=16, Œ±=20)\
‚Ä¢ Targeted 7 key layers (q_proj, v_proj, gate_proj, etc.) for surgical parameter efficiency\
‚Ä¢ Achieved 75% VRAM reduction while retaining diagnostic accuracy on T4 GPU\
‚öôÔ∏è Medical Data Engineering\
‚Ä¢ Processed 19,000 expert-annotated samples from medical-o1-reasoning-SFT\
‚Ä¢ Engineered clinical prompt templates with XML-style reasoning tags\
‚Ä¢ Structured inputs as: Clinical context ‚Üí step-by-step analysis ‚Üí diagnosis/treatment\
ü§ñ Training Optimization & Monitoring\
‚Ä¢ Implemented gradient checkpointing (batch=2 √ó accum=4) for memory efficiency\
‚Ä¢ Trained for 770 steps with AdamW-8bit (lr=2e-4) and linear scheduling\
‚Ä¢ Tracked convergence via Weights & Biases with 10-step logging intervals\
üöÄ Validation & Real-World Application\
‚Ä¢ Validated on trauma cases (e.g. tension pneumothorax diagnosis)\
‚Ä¢ Achieved coherent treatment plans including critical interventions\
‚Ä¢ Saved quantized model to Google Drive for clinical deployment\
üí° Why It Matters: Demonstrates how parameter-efficient fine-tuning unlocks specialized medical reasoning in LLMs - crucial for diagnostic support systems!\
Tools: Python, Hugging Face (TRL, Transformers), Unsloth, LoRA, W&B, 4-bit Quantization

